{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NER for Address Elements Extraction\n\nThe following solution is an attempt to perform Named Entity Recognition (NER) on raw Indonesia addresses. This is my first time dabbling in a data science competition - any suggestions would be greatly appreciated!\n\n# 1. Competition Details\n\n## Background\n\nAt Shopee, we strive to ensure our customers' highest satisfaction for their shopping and delivery experience - fast and accurate delivery of goods. This can be better achieved if we have key address elements for each user address which allows us to accurately geocode it to obtain geographic coordinates to ship the parcel to our customers. These key address elements include Point of Interest (POI) Names and Street Names. However, most addresses that Shopee receives are unstructured and in free text format, not following a certain pattern. Thus it is important for us to develop a model to precisely extract the key address elements from it.\n\n## Task\n\nIn this competition, you’ll work on addresses collected by us to build a model to correctly extract Point of Interest (POI) Names and Street Names from unformatted Indonesia addresses.\n\nParticipants are expected to build their own model for this competition, submissions by teams which directly call any third party APIs on the test set will not be taken into consideration.","attachments":{}},{"metadata":{},"cell_type":"markdown","source":"# 2. Preprocessing\n\n## Labelling\n\nHere, we will label the POI and street entities in the training dataset. The result is a list of raw addresses and their corresponding extracted entities. \n\n## Incomplete Addresses\n\n> The word “pembangunan” in raw_address “smk karya pemban, pon” is not complete. The correct POI will be “smk karya pembangunan” and the returned result should be:\nsmk karya pembangunan/pon\n\nSome raw addresses may be incomplete. We will attempt to map the incomplete address elements to the complete POI / street data in the training dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport re\n\ndf = pd.read_csv('../input/scl-2021-ds/train.csv')\ndf.fillna('', inplace=True)\n\ndf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import string\n\nTRAIN_COMBINED = []\n\ntotal_conflicts = 0\n\ndef find_match(raw_words, target_words, ignore_truncated=True):\n    \n    low = 0\n    high = 0\n    pattern_index = 0\n    found = False\n            \n    while high < len(raw_words):\n\n        word = raw_words[high]\n\n        if pattern_index == len(target_words):\n            found = True\n            break\n            \n        elif not ignore_truncated and (target_words[pattern_index].startswith(word) or target_words[pattern_index].startswith(word[:-1])):\n            high += 1\n            pattern_index += 1\n\n        elif target_words[pattern_index] == word or target_words[pattern_index] == word[:-1]:\n            high += 1\n            pattern_index += 1\n\n        else:\n            low += 1\n            high = low\n            pattern_index = 0\n            \n    if pattern_index == len(target_words):\n        found = True\n\n    if found:\n        vector = [1 if i >= low and i < high else 0 for i in range(len(raw_words))]\n    else:\n        vector = [0 for i in range(len(raw_words))]\n        \n    return vector\n\nfor index, row in df.iterrows():\n    raw_address = row['raw_address']\n    poi, street = row['POI/street'].split('/')\n    \n    combined_entities = []\n    \n    first_entity = None\n    \n    if poi:\n        \n        m = re.search(re.escape(poi), raw_address)\n        \n        if m:\n            low, high = m.start(), m.end()\n        \n        else:\n            \n            vector = find_match(raw_address.split(), poi.split(), False)\n\n            curr_idx = 0\n            low = None\n            high = None\n\n            for i in range(len(vector)):\n                if vector[i] == 1 and low == None:\n                    low = curr_idx\n                elif vector[i] == 0 and low != None and high == None:\n                    high = curr_idx - 1\n                    break\n\n                curr_idx += len(raw_address.split()[i]) + 1\n\n            if low != None and high == None:\n                high = len(raw_address)\n\n        if low != None and high != None:\n\n            while raw_address[high - 1] in string.punctuation:\n                high -= 1\n\n            first_entity = (low, high, \"POI\")\n            combined_entities.append((low, high, \"POI\"))\n            \n    if street:\n        \n        m = re.search(re.escape(street), raw_address)\n        \n        if m:\n            low, high = m.start(), m.end()\n        \n        else:\n        \n            vector = find_match(raw_address.split(), street.split(), False)\n\n            curr_idx = 0\n            low = None\n            high = None\n\n            for i in range(len(vector)):\n                if vector[i] == 1 and low == None:\n                    low = curr_idx\n                elif vector[i] == 0 and low != None and high == None:\n                    high = curr_idx - 1\n                    break\n\n                curr_idx += len(raw_address.split()[i]) + 1\n\n            if low != None and high == None:\n                high = len(raw_address)\n        \n        if low != None and high != None:\n            \n            while raw_address[high - 1] in string.punctuation:\n                high -= 1\n            \n            if first_entity and (\n                # Starting index overlaps with POI tag\n                low >= first_entity[0] and low <= first_entity[1] or \n                \n                # Ending index overlaps with POI tag\n                high >= first_entity[0] and high <= first_entity[1] or\n                \n                # POI tag is a subset of SRT tag\n                low < first_entity[0] and high > first_entity[1]\n            ):\n                # Conflicting tags\n                total_conflicts += 1\n            \n            else:\n                combined_entities.append((low, high, \"SRT\"))\n    \n    combined_spacy_entry = (raw_address, {\"entities\": combined_entities})\n    TRAIN_COMBINED.append(combined_spacy_entry)\n\nprint(f\"{total_conflicts} total conflicts.\")\nprint(TRAIN_COMBINED[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Training\n\nThe NER model is trained using spaCy.\n\n## Dropout\n\nTo prevent overfitting, a random fraction of nodes is dropped according to a given probability. After experimenting, it was found that a decaying dropout rate that starts high and moderates to a lower value as training progresses was most effective."},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nimport random\nimport logging\nlogging.captureWarnings(True)\n\nprint(\"Preparing...\")\n\nnlp = spacy.blank(\"id\") # Indonesian\nner = nlp.create_pipe(\"ner\")\nnlp.add_pipe(ner)\nner.add_label(\"POI\")\nner.add_label(\"SRT\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy.util import minibatch, compounding\nfrom spacy.util import decaying\n\nprint(\"Training...\")\n\n# Start the training\nnlp.begin_training()\n\ndropout = decaying(0.6, 0.2, 1e-4)\n\n# Loop for 20 iterations\nfor itn in range(20):\n    \n    # Shuffle the training data\n    random.shuffle(TRAIN_COMBINED)\n    losses = {}\n    \n    # Batch the examples and iterate over them\n    for batch in spacy.util.minibatch(TRAIN_COMBINED, size=compounding(4.0, 32.0, 1.001)):\n        texts = [text for text, entities in batch]\n        annotations = [entities for text, entities in batch]\n        \n        # Update the model\n        nlp.update(texts, annotations, losses=losses, drop=next(dropout))\n        \n    print(f\"Epoch: {itn}\\tLosses: {str(losses)}\")\n    \nnlp.to_disk(\"combined.model\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Predictions\n\nGenerate predictions on the test dataset.\n\n## Limitations\n\nI've been unable to perform subword completion on incomplete addresses accurately. I've tried to create dictionaries of known vocabulary mappings, e.g. \"smk karya pemban” -> \"smk karya pembangunan\" and expanding the incomplete addresses as a post-processing step, but these experiments have failed to achieve a higher accuracy on the public leaderboard."},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\n\nnlp = spacy.load(\"combined.model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('../input/scl-2021-ds/test.csv')\n\nresults = {}\n\nfor index, row in df.iterrows():\n    raw_address, id = row['raw_address'], row['id']\n    \n    doc = nlp(raw_address)\n    \n    elements = [(X.text, X.label_) for X in doc.ents]\n    poi_elements = [x for x in elements if x[1] == 'POI']\n    srt_elements = [x for x in elements if x[1] == 'SRT']\n    \n    if not poi_elements:\n        poi_elements = [('', '')]\n    \n    if not srt_elements:\n        srt_elements = [('', '')]\n    \n    results[id] = (poi_elements, srt_elements)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Output\n\nGenerate the submission.csv required for the competition."},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('submission.csv', 'w') as f:\n    f.write(\"id,POI/street\\n\")\n    for id in range(50000):\n        poi = results[id][0][0][0]\n        srt = results[id][1][0][0]\n        \n        f.write(f\"{id},\\\"{poi}/{srt}\\\"\\n\")\n\ndf = pd.read_csv('submission.csv')\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Evaluation\n\nWe can evaluate the accuracy of our model by viewing evaluation metrics such as the f-score."},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy.scorer import Scorer\nfrom spacy.gold import GoldParse\nimport logging\nimport random\n\nlogging.captureWarnings(True)\n\ndef evaluate(ner_model, examples):\n    scorer = Scorer()\n    for input_, annot in examples:\n        doc_gold_text = ner_model.make_doc(input_)\n        gold = GoldParse(doc_gold_text, entities=annot.get('entities'))\n        pred_value = ner_model(input_)\n        scorer.score(pred_value, gold)\n    return scorer.scores\n\nrandom.shuffle(TRAIN_COMBINED)\nprint(evaluate(nlp, TRAIN_COMBINED[:100]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}